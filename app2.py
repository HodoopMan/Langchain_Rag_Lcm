import os
import sys
print("Pythonè§£é‡Šå™¨è·¯å¾„ï¼š", sys.executable)
print("æ¨¡å—æœç´¢è·¯å¾„ï¼š", sys.path[:3])  # åªçœ‹å‰3ä¸ªå…³é”®è·¯å¾„
import logging
import streamlit as st
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from dotenv import load_dotenv

# LangChainç›¸å…³å¯¼å…¥
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# ===================== æ ¸å¿ƒé…ç½® =====================
class Config:
    """ä¸­åŒ»RAGé…ç½®ç±»ï¼ˆç¡…åŸºæµåŠ¨åµŒå…¥æ¨¡å‹ç‰ˆï¼‰"""
    DOC_PATH = Path("./docs/ç–¾ç—….txt")
    VECTOR_STORE_DIR = Path("./chroma_db/traditional_chinese_medicine")

    # æ–‡æœ¬åˆ†å‰²é…ç½®
    TEXT_SPLITTER_CONFIG = {
        "chunk_size": 400,
        "chunk_overlap": 40,
        "separators": ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼š", "ï¼Œ", "ã€", "ï¼ˆ", "ï¼‰", "ã€", "ã€‘", "â€”â€”", " ", ""]
    }

    # ç¡…åŸºæµåŠ¨åµŒå…¥æ¨¡å‹é…ç½®
    SILICONFLOW_EMBEDDING_CONFIG = {
        "model": "BAAI/bge-m3",
        "api_key": "sk-tvamliktbquwrcikvlxmwmdkwlumgfnstidlzudbzwmajwgf",
        "base_url": "https://api.siliconflow.cn/v1",
        "chunk_size": 1000,
    }

    # ç¡…åŸºæµåŠ¨LLMé…ç½®
    SILICONFLOW_LLM_CONFIG = {
        "model": "Qwen/Qwen2-7B-Instruct",
        "api_key": "sk-tvamliktbquwrcikvlxmwmdkwlumgfnstidlzudbzwmajwgf",
        "base_url": "https://api.siliconflow.cn/v1",
        "temperature": 0,
        "max_tokens": 2000
    }

    # æ£€ç´¢é…ç½®
    RETRIEVER_CONFIG = {
        "k": 5,
        "search_type": "similarity"
    }

    # ç¡…åŸºæµåŠ¨æ”¯æŒçš„åµŒå…¥æ¨¡å‹åˆ—è¡¨
    EMBEDDING_MODELS = [
        "BAAI/bge-m3",
        "BAAI/bge-large-zh-v1.5",
        "text-embedding-ada-002",
        "moka-ai/m3e-large",
        "intfloat/multilingual-e5-large"
    ]

    # ğŸ”¥ ä¿®å¤ï¼šç®€åŒ–æç¤ºè¯æ¨¡æ¿ï¼ˆç§»é™¤chat_historyå˜é‡ï¼Œç”±Chainè‡ªåŠ¨å¤„ç†ï¼‰
    PROMPT_TEMPLATE = """
    ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ä¸­åŒ»ä¸´åºŠè¯Šç–—ä¸“å®¶ï¼Œç²¾é€šä¸­åŒ»ç†è®ºå’Œä¸´åºŠå®è·µï¼Œå°¤å…¶æ“…é•¿å„ç±»ä¸­åŒ»ç–¾ç—…çš„è¾¨è¯è®ºæ²»ã€‚
    è¯·æ ¹æ®ä»¥ä¸‹è§„åˆ™å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š
    1. å¿…é¡»åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ï¼Œç¡®ä¿ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§ï¼›

    å‚è€ƒæ–‡æ¡£å†…å®¹ï¼š
    {context}

    ç”¨æˆ·å½“å‰é—®é¢˜ï¼š
    {question}
    """

    # ç³»ç»Ÿæç¤ºè¯
    SYSTEM_PROMPT = """
    ä½ æ˜¯ä¸­åŒ»ä¸´åºŠè¯Šç–—RAGåŠ©æ‰‹ï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š
    1. ç²¾å‡†ç†è§£ä¸­åŒ»æœ¯è¯­å’Œä¸´åºŠé—®é¢˜
    2. ç»“åˆå†å²å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œä¿æŒå›ç­”çš„è¿è´¯æ€§
    3. è®°å¿†ç”¨æˆ·ä¹‹å‰çš„æé—®å’Œå…³æ³¨ç‚¹
    4. ç”¨æˆ·è¾“å…¥äº†ä¸ä¸­åŒ»æ— å…³çš„é—®ç›´æ¥å›ç­”:æŠ±æ­‰ï¼Œæˆ‘ä¸“æ³¨äºä¸­åŒ»ä¸´åºŠè¯Šç–—é¢†åŸŸã€‚å¦‚éœ€äº†è§£ä¸­åŒ»ç–¾ç—…çš„è¯Šç–—æ–¹æ³•ï¼Œæˆ‘ä¼šå°½åŠ›è§£ç­”ã€‚
    """

    # ä¸Šä¸‹æ–‡è®°å¿†é…ç½®
    CONTEXT_MEMORY_CONFIG = {
        "max_history_length": 10,
        "history_separator": "\n---\n",
        "context_window_size": 3000
    }


# ===================== å·¥å…·å‡½æ•°ï¼ˆä¿®å¤å¯¹è¯å†å²æ ¼å¼ï¼‰ =====================
def format_chat_history_for_display(chat_history: List[Tuple[str, str]]) -> str:
    """æ ¼å¼åŒ–å¯¹è¯å†å²ç”¨äºå±•ç¤ºï¼ˆä»…å‰ç«¯æ˜¾ç¤ºç”¨ï¼‰"""
    if not chat_history:
        return "æ— "

    # é™åˆ¶å†å²é•¿åº¦
    history_to_use = chat_history[-Config.CONTEXT_MEMORY_CONFIG["max_history_length"]:]

    formatted_history = []
    for i, (human, ai) in enumerate(history_to_use, 1):
        formatted_history.append(f"ç”¨æˆ·{i}ï¼š{human}")
        formatted_history.append(f"åŠ©æ‰‹{i}ï¼š{ai}")

    return Config.CONTEXT_MEMORY_CONFIG["history_separator"].join(formatted_history)


def create_custom_prompt() -> PromptTemplate:
    """åˆ›å»ºè‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿ï¼ˆä¿®å¤ï¼šä»…ä¿ç•™contextå’Œquestionå˜é‡ï¼‰"""
    prompt = PromptTemplate(
        template=Config.PROMPT_TEMPLATE,
        input_variables=["context", "question"]  # ç§»é™¤chat_historyå˜é‡
    )
    return prompt


@st.cache_resource(show_spinner="æ­£åœ¨åŠ è½½ä¸­åŒ»æ–‡æ¡£...")
def load_documents(file_path: Path) -> List[Document]:
    documents = []
    if not file_path.exists():
        st.error(f"æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path.absolute()}")
        return documents

    encodings = ["utf-8", "gbk", "gb2312", "utf-8-sig"]
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                lines = f.readlines()
                doc_text = "".join(lines).strip()

            if not doc_text:
                st.warning("æ–‡æ¡£å†…å®¹ä¸ºç©º")
                return documents

            document = Document(
                page_content=doc_text,
                metadata={"source": str(file_path.absolute()), "file_name": file_path.name,
                          "file_size": file_path.stat().st_size, "encoding": encoding}
            )
            documents.append(document)
            st.success(f"æˆåŠŸåŠ è½½æ–‡æ¡£ï¼š{file_path.name}ï¼ˆç¼–ç ï¼š{encoding}ï¼‰")
            break
        except UnicodeDecodeError:
            continue
        except Exception as e:
            st.error(f"åŠ è½½æ–‡æ¡£å¤±è´¥ï¼š{str(e)}")
            return documents
    return documents


@st.cache_resource(show_spinner="æ­£åœ¨åˆ‡å‰²æ–‡æœ¬...")
def split_documents(documents: List[Document]) -> List[Document]:
    if not documents:
        return []
    text_splitter = RecursiveCharacterTextSplitter(**Config.TEXT_SPLITTER_CONFIG)
    split_docs = text_splitter.split_documents(documents)
    st.info(f"æ–‡æœ¬åˆ‡å‰²å®Œæˆï¼šåŸå§‹{len(documents)}ä¸ªæ–‡æ¡£ â†’ åˆ‡å‰²å{len(split_docs)}ä¸ªæ–‡æœ¬å—")
    return split_docs


@st.cache_resource(show_spinner="æ­£åœ¨åˆå§‹åŒ–ç¡…åŸºæµåŠ¨åµŒå…¥æ¨¡å‹...")
def init_siliconflow_embeddings(model_name: str) -> OpenAIEmbeddings:
    """åˆå§‹åŒ–ç¡…åŸºæµåŠ¨åµŒå…¥æ¨¡å‹"""
    if not Config.SILICONFLOW_EMBEDDING_CONFIG["api_key"]:
        st.error("ç¡…åŸºæµåŠ¨API Keyæœªé…ç½®ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®SILICONFLOW_API_KEY")
        return None

    try:
        embeddings = OpenAIEmbeddings(
            model=model_name,
            api_key=Config.SILICONFLOW_EMBEDDING_CONFIG["api_key"],
            base_url=Config.SILICONFLOW_EMBEDDING_CONFIG["base_url"],
            chunk_size=Config.SILICONFLOW_EMBEDDING_CONFIG["chunk_size"]
        )

        # éªŒè¯æ¨¡å‹
        test_embedding = embeddings.embed_query("æ„Ÿå†’çš„ä¸­åŒ»æ²»ç–—")
        st.success(f"âœ… ç¡…åŸºæµåŠ¨åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼š{model_name}")
        st.info(f"ğŸ“ å‘é‡ç»´åº¦ï¼š{len(test_embedding)}")
        return embeddings

    except Exception as e:
        st.error(f"åˆå§‹åŒ–ç¡…åŸºæµåŠ¨åµŒå…¥æ¨¡å‹å¤±è´¥ï¼š{str(e)}")
        logger.error(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–é”™è¯¯ï¼š{str(e)}", exc_info=True)
        return None


@st.cache_resource(show_spinner="æ­£åœ¨åˆ›å»ºå‘é‡åº“...")
def create_vector_store(split_docs: List[Document], embeddings, reset: bool = False) -> Chroma:
    if not split_docs:
        st.error("åˆ‡å‰²åçš„æ–‡æœ¬å—ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºå‘é‡åº“")
        return None

    if not embeddings:
        st.error("åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œæ— æ³•åˆ›å»ºå‘é‡åº“")
        return None

    # å¼ºåˆ¶é‡ç½®å‘é‡åº“
    Config.VECTOR_STORE_DIR.mkdir(parents=True, exist_ok=True)
    if reset or Config.VECTOR_STORE_DIR.exists():
        import shutil
        try:
            shutil.rmtree(Config.VECTOR_STORE_DIR)
            Config.VECTOR_STORE_DIR.mkdir(parents=True, exist_ok=True)
            st.warning("ğŸ—‘ï¸ å·²é‡ç½®å‘é‡åº“ï¼ˆä¿è¯åµŒå…¥æ¨¡å‹ç»´åº¦ä¸€è‡´ï¼‰")
        except Exception as e:
            st.error(f"é‡ç½®å‘é‡åº“å¤±è´¥ï¼š{str(e)}")
            return None

    try:
        vector_store = Chroma.from_documents(
            documents=split_docs,
            embedding=embeddings,
            persist_directory=str(Config.VECTOR_STORE_DIR)
        )
        vector_store.persist()
        st.success(f"âœ… å‘é‡åº“åˆ›å»ºå®Œæˆï¼ˆå­˜å‚¨è·¯å¾„ï¼š{Config.VECTOR_STORE_DIR}ï¼‰")
        return vector_store
    except Exception as e:
        st.error(f"åˆ›å»ºå‘é‡åº“å¤±è´¥ï¼š{str(e)}")
        return None


# ç›¸ä¼¼åº¦æ£€ç´¢
def get_similar_documents(vector_store: Chroma, query: str, k: int = 5) -> List[Tuple[Document, float]]:
    """è·å–å¸¦ç›¸ä¼¼åº¦çš„æ£€ç´¢ç»“æœ"""
    if not vector_store:
        return []

    similar_docs = vector_store.similarity_search_with_score(
        query=query,
        k=k
    )
    similar_docs.sort(key=lambda x: x[1], reverse=True)
    return similar_docs


# ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šè°ƒæ•´é—®ç­”é“¾åˆ›å»ºæ–¹å¼
@st.cache_resource(show_spinner="æ­£åœ¨åˆå§‹åŒ–ç¡…åŸºæµåŠ¨LLM...")
def create_qa_chain(_vector_store: Chroma) -> ConversationalRetrievalChain:
    """åˆ›å»ºå¸¦ä¸Šä¸‹æ–‡è®°å¿†çš„é—®ç­”é“¾ï¼ˆä¿®å¤å¯¹è¯å†å²æ ¼å¼é—®é¢˜ï¼‰"""
    if not _vector_store:
        return None

    if not Config.SILICONFLOW_LLM_CONFIG["api_key"]:
        st.error("ç¡…åŸºæµåŠ¨API Keyæœªé…ç½®ï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶")
        return None

    try:
        # åˆå§‹åŒ–ç¡…åŸºæµåŠ¨LLM
        llm = ChatOpenAI(**Config.SILICONFLOW_LLM_CONFIG)
        st.success(f"âœ… ç¡…åŸºæµåŠ¨LLMåˆå§‹åŒ–æˆåŠŸï¼š{Config.SILICONFLOW_LLM_CONFIG['model']}")
    except Exception as e:
        st.error(f"åˆå§‹åŒ–LLMå¤±è´¥ï¼š{str(e)}")
        return None

    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = _vector_store.as_retriever(
        search_type=Config.RETRIEVER_CONFIG["search_type"],
        search_kwargs={"k": Config.RETRIEVER_CONFIG["k"]}
    )

    # åˆ›å»ºè‡ªå®šä¹‰æç¤ºè¯
    custom_prompt = create_custom_prompt()

    # ğŸ”¥ ä¿®å¤ï¼šåˆ›å»ºé—®ç­”é“¾ï¼ˆä½¿ç”¨Chainé»˜è®¤çš„chat_historyå¤„ç†æ–¹å¼ï¼‰
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        return_source_documents=True,
        verbose=False,
        # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯
        combine_docs_chain_kwargs={
            "prompt": custom_prompt,
            "document_variable_name": "context"
        },
        # ä¸Šä¸‹æ–‡é…ç½®
        max_tokens_limit=Config.CONTEXT_MEMORY_CONFIG["context_window_size"],
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¯ç”¨é»˜è®¤çš„å¯¹è¯å†å²å¤„ç†
        output_key="answer"
    )

    st.success("âœ… ä¸­åŒ»è¯Šç–—é—®ç­”æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ˆå¸¦ä¸Šä¸‹æ–‡è®°å¿†ï¼‰ï¼")
    return qa_chain


# ===================== Streamlitä¸»ç•Œé¢ =====================
def main():
    st.set_page_config(
        page_title="ä¸­åŒ»RAGåŠ©æ‰‹ï¼ˆç¡…åŸºæµåŠ¨åµŒå…¥ç‰ˆï¼‰",
        page_icon="ğŸ¥",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.title("ğŸ¥ ä¸­åŒ»ä¸´åºŠè¯Šç–—RAGåŠ©æ‰‹")
    st.subheader("âœ¨ åŸºäºç¡…åŸºæµåŠ¨ (SiliconFlow) | å¢å¼ºä¸Šä¸‹æ–‡è®°å¿†")
    st.divider()

    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ ç¡…åŸºæµåŠ¨é…ç½®")

        # API Keyé…ç½®
        api_key = st.text_input(
            "ç¡…åŸºæµåŠ¨API Key",
            value=Config.SILICONFLOW_EMBEDDING_CONFIG["api_key"],
            type="password",
            help="ä»ç¡…åŸºæµåŠ¨æ§åˆ¶å°è·å–ï¼šhttps://siliconflow.cn"
        )
        if api_key:
            Config.SILICONFLOW_EMBEDDING_CONFIG["api_key"] = api_key
            Config.SILICONFLOW_LLM_CONFIG["api_key"] = api_key

        # åµŒå…¥æ¨¡å‹é€‰æ‹©
        embedding_model = st.selectbox(
            "é€‰æ‹©åµŒå…¥æ¨¡å‹",
            Config.EMBEDDING_MODELS,
            index=0,
            help="ç¡…åŸºæµåŠ¨æ‰˜ç®¡çš„åµŒå…¥æ¨¡å‹"
        )

        # LLMæ¨¡å‹é€‰æ‹©
        llm_model = st.selectbox(
            "é€‰æ‹©LLMæ¨¡å‹",
            ["Qwen/Qwen2-7B-Instruct", "Qwen/Qwen2-14B-Instruct", "Meta-Llama-3-8B-Instruct"],
            index=0
        )
        Config.SILICONFLOW_LLM_CONFIG["model"] = llm_model

        # ä¸Šä¸‹æ–‡è®°å¿†é…ç½®
        st.markdown("---")
        st.header("ğŸ§  ä¸Šä¸‹æ–‡è®°å¿†é…ç½®")
        max_history = st.slider(
            "æœ€å¤§è®°å¿†è½®æ•°",
            min_value=3,
            max_value=20,
            value=Config.CONTEXT_MEMORY_CONFIG["max_history_length"],
            help="æ§åˆ¶å¯¹è¯å†å²çš„è®°å¿†é•¿åº¦ï¼Œè¿‡å¤§ä¼šå¢åŠ Tokenæ¶ˆè€—"
        )
        Config.CONTEXT_MEMORY_CONFIG["max_history_length"] = max_history

        # æ–‡æ¡£è·¯å¾„é…ç½®
        doc_path = st.text_input(
            "æ–‡æ¡£è·¯å¾„",
            value=str(Config.DOC_PATH),
            help="ä¸­åŒ»è¯Šç–—æ–‡æ¡£è·¯å¾„ï¼ˆtxtæ ¼å¼ï¼‰"
        )
        Config.DOC_PATH = Path(doc_path)

        # é‡ç½®æŒ‰é’®
        col1, col2, col3 = st.columns(3)
        with col1:
            reset_vector_db = st.button("ğŸ”„ é‡ç½®å‘é‡åº“", type="secondary")
        with col2:
            if st.button("â™»ï¸ æ¸…ç©ºç¼“å­˜", type="secondary"):
                st.cache_resource.clear()
                st.rerun()
        with col3:
            if st.button("ğŸ§¹ æ¸…ç©ºè®°å¿†", type="secondary"):
                st.session_state.chat_history = []
                if "similar_docs_history" in st.session_state:
                    del st.session_state["similar_docs_history"]
                st.success("å·²æ¸…ç©ºå¯¹è¯è®°å¿†ï¼")
                st.rerun()

        # ä½¿ç”¨è¯´æ˜
        st.markdown("---")
        st.info(
            "ğŸ“ ä½¿ç”¨è¯´æ˜ï¼š\n"
            "1. è¾“å…¥ç¡…åŸºæµåŠ¨API Key\n"
            "2. é€‰æ‹©åµŒå…¥æ¨¡å‹å’ŒLLMæ¨¡å‹\n"
            "3. è°ƒæ•´ä¸Šä¸‹æ–‡è®°å¿†è½®æ•°\n"
            "4. ç¡®è®¤æ–‡æ¡£è·¯å¾„æ­£ç¡®\n"
            "5. åœ¨å¯¹è¯æ¡†ä¸­è¾“å…¥ä¸­åŒ»é—®é¢˜\n"
            "6. ç³»ç»Ÿä¼šè®°å¿†å¯¹è¯å†å²ï¼Œæä¾›è¿è´¯å›ç­”"
        )

        # æ³¨æ„äº‹é¡¹
        st.markdown("---")
        st.warning(
            "âš ï¸ æ³¨æ„äº‹é¡¹ï¼š\n"
            "1. åˆ‡æ¢åµŒå…¥æ¨¡å‹ä¼šè‡ªåŠ¨é‡ç½®å‘é‡åº“\n"
            "2. æ¨¡å‹è°ƒç”¨ä¼šäº§ç”Ÿç›¸åº”çš„è®¡è´¹\n"
            "3. ç¡®ä¿API Keyæœ‰å¯¹åº”æ¨¡å‹çš„æƒé™\n"
            "4. è®°å¿†è½®æ•°è¶Šå¤šï¼ŒTokenæ¶ˆè€—è¶Šå¤§"
        )

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []  # æ ¼å¼ï¼šList[Tuple[str, str]]
    if "vector_store" not in st.session_state:
        st.session_state.vector_store = None
    if "qa_chain" not in st.session_state:
        st.session_state.qa_chain = None
    if "embeddings" not in st.session_state:
        st.session_state.embeddings = None
    if "similar_docs_history" not in st.session_state:
        st.session_state.similar_docs_history = []

    # ç³»ç»Ÿåˆå§‹åŒ–æµç¨‹
    with st.spinner("ç³»ç»Ÿåˆå§‹åŒ–ä¸­..."):
        # 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        if not st.session_state.embeddings:
            st.session_state.embeddings = init_siliconflow_embeddings(embedding_model)

        # 2. åŠ è½½æ–‡æ¡£
        raw_docs = load_documents(Config.DOC_PATH)

        # 3. åˆ‡å‰²æ–‡æœ¬
        if raw_docs and st.session_state.embeddings:
            split_docs = split_documents(raw_docs)

            # 4. åˆ›å»ºå‘é‡åº“
            if split_docs and (reset_vector_db or not st.session_state.vector_store):
                st.session_state.vector_store = create_vector_store(
                    split_docs,
                    st.session_state.embeddings,
                    reset=reset_vector_db
                )

            # 5. åˆ›å»ºé—®ç­”é“¾
            if st.session_state.vector_store and not st.session_state.qa_chain:
                st.session_state.qa_chain = create_qa_chain(st.session_state.vector_store)

    # ä¸Šä¸‹æ–‡è®°å¿†å±•ç¤º
    col1, col2 = st.columns([7, 3])

    with col2:
        st.header("ğŸ“œ å¯¹è¯è®°å¿†")
        if st.session_state.chat_history:
            st.info(
                f"å½“å‰è®°å¿†è½®æ•°ï¼š{len(st.session_state.chat_history)}/{Config.CONTEXT_MEMORY_CONFIG['max_history_length']}")

            # æŠ˜å æ˜¾ç¤ºå®Œæ•´å¯¹è¯å†å²
            with st.expander("æŸ¥çœ‹å®Œæ•´å¯¹è¯è®°å¿†", expanded=False):
                formatted_history = format_chat_history_for_display(st.session_state.chat_history)
                st.text_area(
                    "å¯¹è¯å†å²",
                    value=formatted_history,
                    height=300,
                    disabled=True
                )
        else:
            st.info("æš‚æ— å¯¹è¯è®°å¿†ï¼Œè¯·å¼€å§‹æé—®...")

    with col1:
        # èŠå¤©ç•Œé¢
        st.subheader("ğŸ’¬ å¯¹è¯ç•Œé¢")

        # æ˜¾ç¤ºå†å²å¯¹è¯
        for i, (question, answer) in enumerate(st.session_state.chat_history):
            with st.chat_message("user", avatar="ğŸ‘¤"):
                st.markdown(question)
            with st.chat_message("assistant", avatar="ğŸ¥"):
                st.markdown(answer)

                # æ˜¾ç¤ºä¸Šä¸‹æ–‡å…³è”æç¤º
                if i > 0:
                    st.caption(f"ğŸ’¡ å…³è”ä¸Šæ–‡ï¼šç¬¬{i}è½®å¯¹è¯")

                # æ˜¾ç¤ºå‚è€ƒæ–‡æ¡£
                if i < len(st.session_state.get("similar_docs_history", [])):
                    similar_docs = st.session_state["similar_docs_history"][i]
                    if similar_docs:
                        with st.expander(f"ğŸ“Š å‚è€ƒæ–‡æ¡£ï¼ˆç›¸ä¼¼åº¦æ’åºï¼‰", expanded=False):
                            for j, (doc, score) in enumerate(similar_docs, 1):
                                st.markdown(f"### å‚è€ƒæ–‡æ¡£ {j}ï¼ˆç›¸ä¼¼åº¦ï¼š{score:.4f}ï¼‰")
                                st.markdown(f"**æ¥æº**ï¼š{doc.metadata.get('file_name', 'æœªçŸ¥')}")
                                st.markdown(f"**ç¼–ç **ï¼š{doc.metadata.get('encoding', 'æœªçŸ¥')}")
                                st.markdown(f"**å†…å®¹**ï¼š{doc.page_content[:800]}..." if len(
                                    doc.page_content) > 800 else doc.page_content)
                                st.divider()

        # èŠå¤©è¾“å…¥
        if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„ä¸­åŒ»è¯Šç–—é—®é¢˜..."):
            if not st.session_state.qa_chain:
                st.error("âŒ é—®ç­”æ¨¡å‹å°šæœªåˆå§‹åŒ–å®Œæˆï¼Œè¯·æ£€æŸ¥é…ç½®ï¼")
            else:
                with st.chat_message("user", avatar="ğŸ‘¤"):
                    st.markdown(prompt)

                with st.chat_message("assistant", avatar="ğŸ¥"):
                    with st.spinner("ğŸ” æ­£åœ¨æ£€ç´¢å¹¶ç”Ÿæˆå›ç­”ï¼ˆç»“åˆä¸Šä¸‹æ–‡è®°å¿†ï¼‰..."):
                        try:
                            # è·å–ç›¸ä¼¼åº¦ç»“æœ
                            similar_docs = get_similar_documents(st.session_state.vector_store, prompt,
                                                                 k=Config.RETRIEVER_CONFIG["k"])

                            # ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šä¼ é€’æ­£ç¡®æ ¼å¼çš„å¯¹è¯å†å²ï¼ˆList[Tuple[str, str]]ï¼‰
                            # ConversationalRetrievalChain è¦æ±‚ chat_history æ˜¯åˆ—è¡¨å…ƒç»„æ ¼å¼
                            result = st.session_state.qa_chain({
                                "question": prompt,
                                "chat_history": st.session_state.chat_history  # ç›´æ¥ä¼ é€’åŸå§‹æ ¼å¼
                            })

                            # æ˜¾ç¤ºå›ç­”
                            answer = result["answer"].strip()
                            st.markdown(answer)

                            # ä¿å­˜å¯¹è¯å†å²ï¼ˆä¿æŒList[Tuple[str, str]]æ ¼å¼ï¼‰
                            st.session_state.chat_history.append((prompt, answer))
                            st.session_state.similar_docs_history.append(similar_docs)

                            # é™åˆ¶å†å²é•¿åº¦
                            if len(st.session_state.chat_history) > Config.CONTEXT_MEMORY_CONFIG["max_history_length"]:
                                st.session_state.chat_history = st.session_state.chat_history[
                                                                -Config.CONTEXT_MEMORY_CONFIG["max_history_length"]:]
                                st.session_state.similar_docs_history = st.session_state.similar_docs_history[
                                                                        -Config.CONTEXT_MEMORY_CONFIG[
                                                                            "max_history_length"]:]

                            # æ˜¾ç¤ºå‚è€ƒæ–‡æ¡£
                            with st.expander("ğŸ“š å‚è€ƒæ–‡æ¡£ï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰", expanded=False):
                                st.info(f"ğŸ“ åŸºäº {len(similar_docs)} ä¸ªç›¸å…³æ–‡æ¡£ç”Ÿæˆå›ç­”")
                                for i, (doc, score) in enumerate(similar_docs, 1):
                                    st.markdown(f"### å‚è€ƒæ–‡æ¡£ {i}ï¼ˆç›¸ä¼¼åº¦ï¼š{score:.4f}ï¼‰")
                                    st.markdown(f"**æ¥æºæ–‡ä»¶**ï¼š{doc.metadata.get('file_name', 'æœªçŸ¥')}")
                                    st.markdown(f"**æ–‡ä»¶ç¼–ç **ï¼š{doc.metadata.get('encoding', 'æœªçŸ¥')}")
                                    st.markdown(f"**æ–‡ä»¶è·¯å¾„**ï¼š{doc.metadata.get('source', 'æœªçŸ¥')}")
                                    st.markdown(f"**å†…å®¹**ï¼š{doc.page_content[:800]}..." if len(
                                        doc.page_content) > 800 else doc.page_content)
                                    st.divider()

                            # æ˜¾ç¤ºä¸Šä¸‹æ–‡å…³è”æç¤º
                            if len(st.session_state.chat_history) > 1:
                                st.caption("ğŸ’¡ å›ç­”å·²ç»“åˆä¹‹å‰çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œä¿æŒè¯Šç–—å»ºè®®çš„è¿è´¯æ€§")

                        except Exception as e:
                            st.error(f"âŒ å›ç­”ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
                            logger.error(f"é—®ç­”å‡ºé”™ï¼š{str(e)}", exc_info=True)

        # æ¸…ç©ºå¯¹è¯æŒ‰é’®
        col_clear1, col_clear2 = st.columns([1, 9])
        with col_clear1:
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", type="primary"):
                st.session_state.chat_history = []
                st.session_state.similar_docs_history = []
                st.rerun()


if __name__ == "__main__":
    main()